{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25588\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paddle\\io\\reader.py:433: UserWarning: DataLoader with multi-process mode is not supported on MacOs and Windows currently. Please use signle-process mode with num_workers = 0 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#数据处理部分之前的代码，加入部分故起处理的库\n",
    "import paddle\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_process import get_MNIST_dataloader\n",
    "from tools import plot\n",
    "train_loader,_ = get_MNIST_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义多层全连接神经网络\n",
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__ (self):\n",
    "        super(MNIST, self).__init__ ()\n",
    "        #定义两层全连接隐含层，输出维度是10，当前设定隐含节点数为10，可根据任务调整\n",
    "        self.fc1 = Linear(in_features=784, out_features=10)\n",
    "        self.fc2 = Linear(in_features=10, out_features=10)\n",
    "        #定义一层全连接输出层，输出维度是1\n",
    "        self.fc3 = Linear(in_features=10,out_features=1)\n",
    "    #定义网络的前向计算，隐含层激活函教为sigmoid，输出层不使用激活函数\n",
    "    def forward(self, inputs):\n",
    "        inputs = paddle.reshape(inputs, [inputs.shape[0], 784])\n",
    "        outputs1 = self.fc1(inputs)\n",
    "        outputs1 = F.sigmoid(outputs1)\n",
    "        outputs2 = self.fc2(outputs1)\n",
    "        outputs2 = F.sigmoid(outputs2)\n",
    "        outputs_final = self.fc3(outputs2)\n",
    "        return outputs_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Linear-1          [[1, 784]]            [1, 10]             7,850     \n",
      "   Linear-2          [[1, 10]]             [1, 10]              110      \n",
      "   Linear-3          [[1, 10]]              [1, 1]              11       \n",
      "===========================================================================\n",
      "Total params: 7,971\n",
      "Trainable params: 7,971\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.03\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "{'total_params': 7971, 'trainable_params': 7971}\n"
     ]
    }
   ],
   "source": [
    "model = MNIST()\n",
    "params_info = paddle.summary(model, (1, 1, 28, 28))\n",
    "print(params_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "(InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.\n  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ..\\paddle\\phi\\core\\dense_tensor.cc:166)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_list\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m MNIST()\n\u001b[1;32m---> 33\u001b[0m loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     16\u001b[0m predicts \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#计算损失，取一个批次样本损失的平均值\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare_error_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m paddle\u001b[38;5;241m.\u001b[39mmean(loss)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#每训练200批次的数据，打印下当前Loss的情况\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25588\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\paddle\\nn\\functional\\loss.py:422\u001b[0m, in \u001b[0;36msquare_error_cost\u001b[1;34m(input, label)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03mThis op accepts input predictions and target label and returns the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    419\u001b[0m \n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_dynamic_mode():\n\u001b[1;32m--> 422\u001b[0m     minus_out \u001b[38;5;241m=\u001b[39m \u001b[43m_C_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m     square_out \u001b[38;5;241m=\u001b[39m _C_ops\u001b[38;5;241m.\u001b[39msquare(minus_out)\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m square_out\n",
      "\u001b[1;31mValueError\u001b[0m: (InvalidArgument) The type of data we are trying to retrieve (int64) does not match the type of data (float32) currently contained in the container.\n  [Hint: Expected dtype() == phi::CppTypeToDataType<T>::Type(), but received dtype():10 != phi::CppTypeToDataType<T>::Type():9.] (at ..\\paddle\\phi\\core\\dense_tensor.cc:166)\n"
     ]
    }
   ],
   "source": [
    "#网络结构部分之后的代码，保持不变\n",
    "def train(model):\n",
    "    model.train()\n",
    "    # 使用SGD优化器，learning_rate设置为0.01\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n",
    "     # 训练5轮\n",
    "    EPOCH_NUM = 10\n",
    "    loss_list = []\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            Iabels = paddle.to_tensor(labels, dtype=\"float32\")\n",
    "            #前向计算的过程\n",
    "            predicts = model(images)\n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = F.square_error_cost(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            #每训练200批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                loss_list.append(avg_loss.numpy()[0])\n",
    "                print(\"epoch: ({), batch: [), loss is: ()\".format(epoch_id, batch_id, avg_loss.numpy()))\n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            #最小化loss,更新参数\n",
    "            opt.step()# 清除梯度\n",
    "            opt.clear_grad()\n",
    "    #保存模型参数\n",
    "    paddle.save(model.state_dict(), \"C:\\\\Users\\\\25588\\\\Desktop\\\\python程序设计\\\\shendu\")\n",
    "    return loss_list\n",
    "model = MNIST()\n",
    "loss_list = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多卷积神经网络实现\n",
    "class MNIST(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__ ()\n",
    "       # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "        self.conv1 = (Conv2D(in_channels=1, out_channels=28, kernel_size=5, stride=1, padding=2))\n",
    "        #定义池化层，池化校的大小kernel_size为2，池化步长为2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2,stride=2)\n",
    "        # 定义卷职层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "        self.conv2 = Conv2D(in_channels=20, out_channels=28, kernel_size=5, stride=1, padding=2)\n",
    "        #开定义池化层，池化校核的大小kernel_size为2，池化步长为2\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        #定义一层全连接层，输出维度是1\n",
    "        self.fc = Linear(in_features=980, out_features=1)\n",
    "    #定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连按层计算最终输出#卷积层激活函数使用Relu，全连越层不使用激活函数\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0, -1]])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST()\n",
    "params_info = paddle.summary(model,(1, 1, 28,28))\n",
    "print(params_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络结构部分之后的代码，保持不变\n",
    "def train(model):\n",
    "    model.train()\n",
    "    learning_rate = 0.001\n",
    "    # 使用SGD优化器，设置learning_rate\n",
    "    opt = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.paraneters())\n",
    "    # 训练5轮\n",
    "    EPOCH_NUM = 10\n",
    "    # MNIST图像高和宽\n",
    "    IMG_ROWS, IMG_COLS = 28, 28\n",
    "    loss_list = []\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据\n",
    "            images, labels = data\n",
    "            images = paddle,to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels, dtype=\"float32\")\n",
    "            #前向计算的过程\n",
    "            predicts = model(images) # [batch_size, 1]\n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = F.square_error_cost(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            #每训练200批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                loss_list.append(avg_loss .numpy()[0])\n",
    "                print(\"epoch: (), batch: (), loss is: ()\".format(epoch_id, batch_id, avg_loss.numpy()))\n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            # 最小化loss,更新参数\n",
    "            opt.step()\n",
    "            # 清除梯度\n",
    "            opt.clear_grad()\n",
    "    #保存模型参数\n",
    "    paddle.save(model.state_dict(), 'D:/datasets/pics/projects/digitalRecognition/mnistN2.pdparams')\n",
    "    return loss_list\n",
    "rodel = MNIST()\n",
    "loss_list_conv=train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_losses(loss_list_1, loss_list_2):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    freqs = [i for i in range(len(loss_list_1))]\n",
    "    # 绘制训练损失变化曲线\n",
    "    plt.plot(freqs, loss_list_1, color='#e4007f', label=\"Train loss1\")\n",
    "    plt.plot(freqs, loss_list_2, color='#f19ec2',linestyle='--', label=\"Train loss2\")\n",
    "    #绘制坐标轴和图例fontsize='large')\n",
    "    plt.ylabel(\"loss\", fontsize = 'large')\n",
    "    plt.xlabel(\"freq\", fontsize = 'large')\n",
    "    plt.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.show()\n",
    "plot two_losses(loss_list, loss_list_conv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
